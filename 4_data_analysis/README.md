# Data Analysis: Detecting Fake Job Posts

## NLP Analysis of Job Postings

## Introduction

Fraudulent job postings have become an increasing problem in online job markets,
making it crucial to identify scams before they harm job seekers. This project uses
Natural Language Processing (NLP) to detect deceptive job ads by analyzing linguistic
patterns that separate real postings from fake ones. By examining key textual
features, we can uncover whether a job posting was likely written by a human scammer
or generated by AI—helping job seekers avoid potential fraud.

### Core Research Question  

**Can quantitative linguistic analysis combined with machine learning reliably
differentiate between:**  

1. Authentic human-written job postings  
2. Human-crafted fraudulent job postings  
3. AI-generated fraudulent job postings?

## Analysis Notebooks

### [N-Gram Analysis](notebooks/[n_gram_analysis.ipynb](http://github.com/MIT-Emerging-Talent/ET6-CDSP-group-21-repo/blob/main/4_data_analysis/n-gram.ipynb))

**Methodology:**

- Examined frequency patterns of word sequences (1-3 grams)
- Key findings:
  - Real posts showed consistent use of job-specific terminology
  - Human scams contained 42% more urgency-related bigrams
  - AI-generated posts exhibited unnatural n-gram distributions
- Processed text using NLTK for tokenization
- Compared n-gram profiles across post types using χ² tests
**Key Observations:**
- Real posts showed stable n-gram distributions (e.g., "project management skills"
appeared consistently)
- Human scams contained outlier bigrams ("quick hiring", "no experience")
- AI posts exhibited unusual trigram fluency ("demonstrate robust cross-functional")

**Alternative Approach:**
Could have used word embeddings (GloVe) instead of pure frequency counts, but n-grams
provided better interpretability for our specific scam markers.

### [TF-IDF Analysis](notebooks/[tfidf_analysis.ipynb](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-21-repo/tree/main/4_data_analysis/tfidfinsights))

- Calculated term importance scores across post types
- Notable results:
  - Identified 17 statistically significant scam markers (p<0.01)
  - AI posts showed 28% higher average IDF scores for corporate jargon
  - Developed custom stopword list for job posting domain

### [Readability Assessment](notebooks/[readability_analysis.ipynb](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-21-repo/blob/main/4_data_analysis/readability.ipynb))

**Libraries Selected:**

- TextStat (Flesch, SMOG)
- SyntaxNet (sentence complexity)
- Chose these for:
  - Established academic validation
  - Multi-metric consensus
  - Sentence-level granularity

**Unexpected Result or Critical Insight:**
Interestingly, AI-written posts tend to use more complex vocabulary (scoring higher
in grade level) but are actually harder to understand (lower in readability).
This contradiction highlights their unnatural writing style.

### [Emotional Tone Detection](notebooks/[tone_analysis.ipynb](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-21-repo/blob/main/4_data_analysis/urgency_tone_analysis.ipynb))

**Analysis Flow:**

1. VADER for urgency detection
2. LIWC for psychological markers
3. Custom regex patterns for scam phrases

- Human scams showed:
  - 37% higher emotional intensity
  - 5x more urgency indicators
- AI posts maintained neutral but unnatural tone

**Why This Approach:**
Combined methods overcome individual limitations:

- VADER detects explicit urgency
- LIWC catches subtle emotional cues
- Regex finds known scam patterns

**Fallback Option:**
Had initial plans to use BERT sentiment but found traditional methods more
interpretable for our specific need.

### [POS Tagging Analysis](notebooks/[pos_analysis.ipynb](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-21-repo/blob/main/1_datasets/cleaned_data/llm_refined_fake_posts2.csv))

**Objective:**  
Identify grammatical patterns distinguishing authentic posts from human-written and
AI-generated scams through part-of-speech distributions.

- Analyzed part-of-speech distributions:
  - AI posts contained excessive nominalizations
  - Real posts showed balanced syntactic patterns

**Key Observations:**

- Structural Differences:
  - AI posts showed rigid grammatical templates
  - Human scams had erratic punctuation patterns
  - Real posts exhibited natural variation

**Technical Stack:**

- Stanza for accurate tagging
- Custom noun/verb ratio metrics
- Dependency tree analysis

**Key Insight:**
Human scams used 22% more imperative verbs ("apply now") while AI overused
nominalizations ("the demonstration of skills").

### [Aegean-Hypatia Dataset Analysis](notebooks/[aegean_hypatia_datasets_analysis.ipynb](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-21-repo/blob/main/4_data_analysis/aegean_hypatia_datasets_analysis.ipynb))

- Conducted comparative analysis between original Aegean dataset and LLM-refined
versions
- Used cosine similarity to measure textual divergence
- Applied HDBSCAN clustering for department/salary patterns

**Methodology:**

- Comparative analysis of original vs LLM-refined posts
- Cosine similarity metrics for section-by-section comparison
- HDBSCAN clustering for salary/department patterns

**Key Findings:**

- LLM refinement increased corporate jargon by 28% while preserving scam intent
- Requirements sections showed strongest divergence (cosine similarity: 0.39)
- Benefits sections unexpectedly mimicked authentic posts (similarity: 0.90)

**Critical observation**  
The perfect BERT performance suggests transformer models may capture subtle semantic
patterns beyond what traditional features provide.

## Key Connections to Research

Our NLP approach directly supports scam detection by:

1. Quantifying linguistic anomalies in fraudulent posts
2. Establishing measurable differences between human and AI scams
3. Providing automated detection features for model development

## Key Findings  

### Supporting Evidence for Research Question  

1. **AI-generated scams** are detectable through:  
   - Unnatural word distributions (TF-IDF)  
   - Overly balanced syntactic structures  
   - Lack of domain-specific named entities  

2. **Human scams** exhibit:  
   - Higher emotional valence (p<0.01)  
   - Urgency markers ("apply immediately")  
   - Simpler language complexity  

3. **Authentic posts** maintain:  
   - Consistent professional tone  
   - Role-specific terminology  
   - Balanced readability (grade 10-12)  
